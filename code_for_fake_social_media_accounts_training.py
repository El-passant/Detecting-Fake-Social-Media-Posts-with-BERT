# -*- coding: utf-8 -*-
"""Code for Fake Social Media Accounts training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1af_xF9dIK8ae6q1LXVwU73qIpkpDrHjl
"""

import torch
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import TensorDataset, DataLoader, RandomSampler
from transformers import AdamW
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix
import pandas as pd
import numpy as np
import random as random

# Step 0: Read data from CSV for training and validation
main_df = pd.read_csv("SMDataset-test.csv")  # Replace "main_dataset.csv" with the path to your main dataset CSV file

# # Dictionary to store counts of each class
# class_counts = {}

# indices_to_keep = []

# # Iterate over each class
# for class_label in main_df['class'].unique():
#     # Get the indices of rows with the current class
#     class_indices = main_df[main_df['class'] == class_label].index

#     # Shuffle the indices
#     class_indices = class_indices.tolist()
#     random.shuffle(class_indices)

#     # Keep only 125 indices
#     class_indices_to_keep = class_indices[:125]

#     # Update the dictionary with the counts
#     class_counts[class_label] = len(class_indices_to_keep)

#     indices_to_keep.extend(class_indices_to_keep)



# # Filter the dataframe to keep only 125 data points of each class
# filtered_df = main_df[main_df.index.isin(indices_to_keep)]

# #Save the filtered data
# main_df = filtered_df

main_sentences = main_df["text"].tolist()
main_labels = main_df["class"].tolist()



# Split the main dataset into training and validation sets
train_sentences, val_sentences, train_labels, val_labels = train_test_split(main_sentences, main_labels, test_size=0.1, random_state=42)

# Step 0: Read data from CSV for testing
test_df = pd.read_csv("SMDataset-train.csv")  # Replace "test_dataset.csv" with the path to your test dataset CSV file
test_sentences = test_df["text"].tolist()
test_labels = test_df["class"].tolist()

# Step 1: Preprocess Data for training
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
# Tokenize texts for training
train_tokenized_texts = [tokenizer.tokenize(sent) for sent in train_sentences]

# Create input tensors with corresponding attention masks for training
train_input_ids = []
train_attention_masks = []
for tokens in train_tokenized_texts:
    encoded_dict = tokenizer.encode_plus(tokens, add_special_tokens=True, max_length=128, pad_to_max_length=True, return_attention_mask=True, return_tensors='pt')
    train_input_ids.append(encoded_dict['input_ids'])
    train_attention_masks.append(encoded_dict['attention_mask'])

# Convert lists to tensors for training
train_input_ids = torch.cat(train_input_ids, dim=0)
train_attention_masks = torch.cat(train_attention_masks, dim=0)
train_labels = torch.tensor(train_labels)

# Step 1: Preprocess Data for validation
val_tokenized_texts = [tokenizer.tokenize(sent) for sent in val_sentences]

# Create input tensors with corresponding attention masks for validation
val_input_ids = []
val_attention_masks = []
for tokens in val_tokenized_texts:
    encoded_dict = tokenizer.encode_plus(tokens, add_special_tokens=True, max_length=128, pad_to_max_length=True, return_attention_mask=True, return_tensors='pt')
    val_input_ids.append(encoded_dict['input_ids'])
    val_attention_masks.append(encoded_dict['attention_mask'])

# Convert lists to tensors for validation
val_input_ids = torch.cat(val_input_ids, dim=0)
val_attention_masks = torch.cat(val_attention_masks, dim=0)
val_labels = torch.tensor(val_labels)

# Step 2: Load BERT Model
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=4)  # Assuming 4 labels, change accordingly

# Step 3: Fine-tune BERT Model
batch_size = 32
train_data = TensorDataset(train_input_ids, train_attention_masks, train_labels)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

optimizer = AdamW(model.parameters(), lr=2e-5)
epochs = 8
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for batch in tqdm(train_dataloader, desc="Epoch {}".format(epoch + 1)):
        b_input_ids, b_input_mask, b_labels = batch
        optimizer.zero_grad()
        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)
        loss = outputs.loss
        total_loss += loss.item()
        loss.backward()
        optimizer.step()

# Step 4: Define Validation Inputs, Masks, and Labels
val_data = TensorDataset(val_input_ids, val_attention_masks, val_labels)
val_sampler = RandomSampler(val_data)
val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)

# Step 5: Evaluate Model on Validation Data
model.eval()
val_preds = []
val_labels_list = []
for batch in val_dataloader:
    b_input_ids, b_input_mask, b_labels = batch
    with torch.no_grad():
        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)
    logits = outputs.logits
    _, predicted_labels = torch.max(logits, 1)
    val_preds.extend(predicted_labels.tolist())
    val_labels_list.extend(b_labels.tolist())

val_accuracy = accuracy_score(val_labels_list, val_preds)
print("Validation Accuracy: {:.2f}".format(val_accuracy))

conf_matrix = confusion_matrix(val_labels_list, val_preds)
print("Confusion Matrix:")
print(conf_matrix)

# Step 6: Define Testing Inputs, Masks, and Labels
test_tokenized_texts = [tokenizer.tokenize(sent) for sent in test_sentences]

# Create input tensors with corresponding attention masks for testing
test_input_ids = []
test_attention_masks = []
for tokens in test_tokenized_texts:
    encoded_dict = tokenizer.encode_plus(tokens, add_special_tokens=True, max_length=128, pad_to_max_length=True, return_attention_mask=True, return_tensors='pt')
    test_input_ids.append(encoded_dict['input_ids'])
    test_attention_masks.append(encoded_dict['attention_mask'])

# Convert lists to tensors for testing
test_input_ids = torch.cat(test_input_ids, dim=0)
test_attention_masks = torch.cat(test_attention_masks, dim=0)
test_labels = torch.tensor(test_labels)

# Step 7: Evaluate Model on Testing Data
test_data = TensorDataset(test_input_ids, test_attention_masks, test_labels)
test_sampler = RandomSampler(test_data)
test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)

# Step 8: Evaluate Model on Testing Data
model.eval()
test_preds = []
test_labels_list = []
for batch in test_dataloader:
    b_input_ids, b_input_mask, b_labels = batch
    with torch.no_grad():
        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)
    logits = outputs.logits
    _, predicted_labels = torch.max(logits, 1)
    test_preds.extend(predicted_labels.tolist())
    test_labels_list.extend(b_labels.tolist())

test_accuracy = accuracy_score(test_labels_list, test_preds)
print("Test Accuracy: {:.2f}".format(test_accuracy))

conf_matrix = confusion_matrix(test_labels_list, test_preds)
print("Confusion Matrix:")
print(conf_matrix)

# Install the datasets library if it's not installed
!pip install datasets

# Mount Google Drive to access and save files
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
from datasets import load_dataset

# Load the dataset
dataset = load_dataset("anismahmahi/10_propaganda_techniques_train")

# Convert the dataset to a Pandas DataFrame
df = pd.DataFrame(dataset['train'])  # Assuming you want to use the 'train' split

# Define the file path on Google Drive to save the CSV file
file_path = "/content/drive/My Drive/propaganda.csv"  # Change "dataset.csv" to your desired file name and path

# Save the DataFrame as a CSV file
df.to_csv(file_path, index=False)

# Print confirmation message
print("CSV file saved successfully to:", file_path)

# Install Kaggle API
!pip install kaggle

# Upload Kaggle API Key
from google.colab import files
files.upload()  # Select the kaggle.json file you downloaded

# Move API Key
!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Download Dataset
!kaggle datasets download -d 'uciml/sms-spam-collection-dataset'

# Extract Dataset
import zipfile
with zipfile.ZipFile("sms-spam-collection-dataset.zip", "r") as zip_ref:
    zip_ref.extractall("sms-spam-collection-dataset")

# Copy Dataset to Google Drive
import shutil
shutil.copyfile("sms-spam-collection-dataset/spam.csv", "/content/drive/My Drive/spam.csv")

# Load Dataset
import pandas as pd
df = pd.read_csv("sms-spam-collection-dataset/spam.csv", encoding='latin-1')